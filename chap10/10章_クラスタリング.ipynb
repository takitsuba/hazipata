{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 今日の流れ\n",
    "* 前半（1時間）\n",
    "  * はじめに\n",
    "  * 10.1. 類似度と非類似度\n",
    "  * 10.2. 非階層型クラスタリング\n",
    "  * 10.3. 階層型クラスタリング\n",
    "* 休憩（5分）\n",
    "* 後半（1時間）\n",
    "  * 10.4. 確率モデルによるクラスタリング\n",
    "* 質問、補足、アドバイス等あれば随時お願いします！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# はじめに\n",
    "* 前章まで：教師データあり\n",
    "* 本章　　：教師データなし\n",
    "\n",
    "## クラスタリング\n",
    "* 入力データ間の「類似度」、または「非類似度」を手がかりにデータをいくつかのグループにグループ分けする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# クラスタリングは大きく2つに分けられる\n",
    "## 各データはどれか1つのクラスタのみに属するクラスタリング\n",
    "\n",
    "* 非階層的手法\n",
    "* 階層的手法\n",
    "\n",
    "## データが確率分布に従い、全体をそれらの混合分布で表現するクラスタリング\n",
    "* 混合正規分布（確率分布に正規分布を仮定）\n",
    "  * EMアルゴリズム：学習データからここの正規分布のパラメータとそれらの混合比を求める手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10.1. 類似度と非類似度\n",
    "## 10.1.1.距離の公理\n",
    "* クラスタリングは、データやクラスタ間の類似度・あるいは非類似度に従って、似た者同士をクラスタにする。\n",
    "* データやクラスタ間の類似度を図る基本的な尺度が距離。\n",
    "* ある尺度が距離であるには、次の公理を満たす必要がある。\n",
    "### 距離の公理\n",
    "* 非負性：0以上であること\n",
    "* 反射律：距離が0になるのは二つのものが一致していること\n",
    "* 対称性：二つの距離は順番入れ替えても一緒\n",
    "* 三角不等式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10.1.2. ミンコフスキー距離\n",
    "* よく使われる距離はユークリッド距離を考える。\n",
    "$$ d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\left( \\sum_{k = 1}^d |x_{ik} - x_{jk}|^2 \\right)^{1/2}$$\n",
    "* これはミンコフスキー距離の1種。\n",
    "* 係数の2と(1/2)を自由に変化させて一般化させたのがミンコフスキー距離。\n",
    "$$ d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\left( \\sum_{k = 1}^d |x_{ik} - x_{jk}|^a \\right)^{1/b} $$\n",
    "\n",
    "* aは個々の特徴間の差に対する重み。\n",
    "* bは個々の特徴間の差のa乗の和に対する重み。\n",
    "* このaとbを変化させることで、代表的な距離を表現することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 代表的な距離\n",
    "* 市街地距離\n",
    "* ユークリッド距離\n",
    "* ユークリッド距離の2乗\n",
    "* チェビシェフ距離\n",
    "  * 無限乗することで、最も大きい差以外無視できるようになる。そして無限分の1乗することで、最大距離そのものに戻る。\n",
    "* （どんな時にどの距離を用いればいいかわかってない・・・）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 類似度を図るその他の代表的尺度\n",
    "* キャンベラ尺度\n",
    "  * データを正規化する仕組みを入れた尺度\n",
    "\n",
    "\n",
    "* 方向余弦\n",
    "  * ベクトル間の角度を用いた尺度でありよく利用される。\n",
    "  * 言い換えるとコサイン類似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10.2. 非階層クラスタリング\n",
    "* k-means\n",
    "* d次元のN個のデータを、データ間の類似性（距離）を尺度に、あらかじめ定めたK個のクラスタに分類すること。\n",
    "\n",
    "## 評価関数\n",
    "$$ J(q_{ik}, \\boldsymbol{\\mu}_k)\n",
    "\t\t\t\t\t= \\sum_{i = 1}^N \\sum_{k = 1}^K q_{ik} || \\boldsymbol{x}_i - \\boldsymbol{\\mu}_k || ^2 $$\n",
    "                    \n",
    "* 「各xと、そのxが帰属するクラスタの代表ベクトルの距離」の2乗の合計。これを最小にしたい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 最適な$\\boldsymbol{\\mu}_k$の決め方\n",
    "$$ \\frac{\\partial J(q_{ik}, \\boldsymbol{\\mu}_k)}{\\partial \\boldsymbol{\\mu}_k}\n",
    "\t\t\t\t\t= 2 \\sum_{i = 1}^N q_{ik} (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_i)\n",
    "\t\t\t\t\t= 0\n",
    "\t\t\t\t\t\\Rightarrow \\boldsymbol{\\mu}_k\n",
    "\t\t\t\t\t= \\frac{\\sum_{i = 1}^N q_{ik} \\boldsymbol{x}_i}{\\sum_{i = 1}^N q_{ik}} $$\n",
    "                    \n",
    "* 代表ベクトルは、そのクラスタに属するデータの平均ベクトルであれば良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $q_{ik}$と$\\boldsymbol{\\mu}_k$を同時に最適化するのは難しい\n",
    "* 同時に最適化するのは難しい。そこでk-meansでは逐次最適化の手順で行う。\n",
    "* (1)　は式10.3 において、μを固定し、帰属変数qを最適にする。（もっとも近い代表ベクトルμのカテゴリに振り分ける）\n",
    "* (2) μの最適化：10.4に従って、μを平均ベクトルとする。\n",
    "* (3) これを繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  注意点\n",
    "* 最適化アルゴリズムの収束先は初期値に依存\n",
    "* 最適解に近い解を得るためには、何回か初期値を変えて実行する必要がある。\n",
    "* 各代表ベクトルが支配する領域は、代表ベクトル間のボロノイ境界で決まる。（p55)\n",
    "\n",
    "\n",
    "## 類似手法\n",
    "* k-medoids\n",
    "* 代表ベクトルをデータベクトルに限った方法\n",
    "* 非類似度の尺度ではかられた量であれば、距離である必要はない（距離でも良い）\n",
    "* k-meansは距離の2乗、k-medoidsは1乗。そのため、比較的k-medoidsの方が外れ値の影響が少ない、ロバストな方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10.3. 階層型クラスタリング（融合法）\n",
    "* N個のデータから、類似度の高い順に融合して次第に大きなクラスタを作り、最終的にはN個のデータを1つのクラスタに統合する手法。\n",
    "\n",
    "## アルゴリズム\n",
    "1. n = N とする（nこのクラスタを作る）\n",
    "2. n x nの距離行列を作る。（p159の表10.1を参照。）\n",
    "  * n個のクラスについて、全ての組み合わせの距離を表にする。\n",
    "  * 対角線上は0、対角線上を鏡とすると、反対側には同じ距離。（距離の公理）\n",
    "3. 最も距離が近い2つのデータやクラスタをまとめて1つのクラスタにする。\n",
    "4. n=n-1にする（2つのクラスタが1つになったから、クラスタの全数は1つ減る。）\n",
    "5. n> 1であれば(2), n=1であれば終了。（つまり、クラスタ数が1になるまで（全てを1つのクラスタに統合するまで）これを続ける。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 類似度の測り方\n",
    "* データ間の類似度の尺度：ミンコフスキー距離、キャンベラ尺度、方向余弦を用いる。\n",
    "* クラスタ間の類似度の尺度：データ間の類似度の尺度と同じもの。\n",
    "* クラスタ間の類似度の定義：様々なものがあるので以下説明。\n",
    "  * 類似度を測るのにどの尺度を用いてもいいが、どことどこを測るかに違いがある。\n",
    "  * 単連結法\n",
    "  * 完全連結法\n",
    "  * 群平均法\n",
    "  * ウォード法\n",
    "  * 重心法\n",
    "  * メディアン法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.3.1.単連結法\n",
    "* 二つのクラスタA,B間でもっとも類似度の高いデータ間の距離をクラスタの距離にする方法\n",
    "* 要は、（距離を尺度とするなら）2つのクラスタから最も近い2つを持ってきて、その距離を二つのクラスタ間の距離とする。\n",
    "\n",
    "### 流れ\n",
    "* クラスタ間の距離を測る。\n",
    "* 一番近い距離がわかったら、その2つのクラスタを融合する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 単連結法の性質\n",
    "* クラスタに1つデータが追加されると、他のクラスタとの距離は小さくなるか、または変化しない。（「最小の距離」しか見てないので、遠いところにデータが追加されても最小の距離は変わらないし、より近いところにデータが追加されれば、最小の距離はそちらになる）\n",
    "* クラスタAとBが融合されてCになったら、他のクラスタXとCとの距離は、XとA、XとBの短い方の距離になる。\n",
    "* 大きなクラスタができる傾向がある（完全連結法との比較を後で考える）\n",
    "* あるクラスタから同じ距離に2つのクラスタがある場合、どちらを選んでも結果は同じ（どちらが先に融合しても、次に融合するのはもう一方のクラスタだからか？）\n",
    "* 近いデータが別なクラスタに属する連鎖効果が現れる場合（最小距離のものがくっついていきやすい）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.3.2.超距離（単連結法で成り立つ性質の説明）\n",
    "### 定義\n",
    "* 2つのデータがあり、それぞれ別のクラスタに属している。\n",
    "* その2つのクラスタが融合する直前の、クラスタ間の距離を超距離と呼ぶ。\n",
    "\n",
    "### 性質\n",
    "1. 二つのデータの距離よりその2つのデータの所属するクラスタの距離の方が短い\n",
    "  * クラスタにはより近い点も含まれてるかもしれないから。\n",
    "2. ある他の1点(x_k)を取った時、2つの点の属するクラスタとその点の属するクラスタとの距離の合算は、2つの点のクラスタの距離以上。\n",
    "  * ある他の1点が、2つの点の属するクラスタ外であれば当然長くなる。ある他の1点が、2つの点の属するクラスタのどちらかにあれば、等しくなる。\n",
    "3. 超距離不等式\n",
    "  * (2)同様、ある他の点x_kが、2つの点の属するクラスタ外であれば、距離長い。\n",
    "  * x_kがどちらかのクラスタに属していれば、右辺の1項目は距離0、もう1項目が2つのクラスタの距離と等しくなり、イコールが成り立つ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.3.3.完全連結法\n",
    "* 単連結法：最近傍距離を基準\n",
    "* 完全連結法：最遠隣距離を基準\n",
    "* クラスタ間でもっとも類似度の低いデータ間の距離を、「クラスタ間の距離」とみなす方法\n",
    "* 図の10.6参照。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 完全連結法の性質\n",
    "* クラスタに1つデータが追加されると、他のクラスタとの距離は大きくなるか、または変化しない。\n",
    "* 2つのクラスタが融合された時、他のクラスタとの距離は、元のクラスタの距離の長い方になる。\n",
    "* 大きなクラスタになりにくく、同じようなサイズのクラスタができる傾向がある。\n",
    "  * データがくっつけばくっつくほど、他のデータとくっつきづらくなる（遠いデータがそのクラスタに含まれるから）。一方単距離は、くっつけばくっつくほど、他のデータとの距離が小さくなる\n",
    "* 連鎖効果は現れない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.3.4.群平均法\n",
    "* 二つのクラスタ内の全てのデータ対間の距離の平均でクラスタ間の距離を決める方法\n",
    "* あるクラスタAとBがあり、クラスタAにはp,q,r、Bにはs,t,uのデータがあるとする。\n",
    "* その時、クラスタAとBの距離は、pとsの距離、pとtの距離、・・・・rとuの距離、の平均とする。(10.8)\n",
    "* 上の例であれば3点x3点=9点間の距離を計算する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.3.5. ウォード法\n",
    "* あるクラスタについて考える。そのクラスタの平均ベクトルをμとする。\n",
    "* クラスタに属する1つ1つのデータとμのユークリッド距離の2乗和を足し上げたものが「クラスタ内変動」である。\n",
    "* 2つのクラスタがあるとき、「2つを結合した後の変動」と、「2つのクラスタ単体の変動の合計」を比較し、その増加分を「2つのクラスタの距離」とする。\n",
    "* 距離が短いもの（変動が増加しにくいもの）から結合していくのがウォード法。\n",
    "\n",
    "### 精度\n",
    "階層法の中でも最も精度が高い方法（よくわからない）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## その他\n",
    "* 重心法：クラスタの重心の差の2乗で距離を定義\n",
    "* メディアン法：中央値を用いる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 10.4. 確率モデルによるクラスタリング\n",
    "* ここまでの話:ハードクラスタリング（1つのデータは1つのクラスタにのみ分類される）\n",
    "* ここからの話:データ分布に確率モデルを当てはめ、どのクラスタに属するかは確率的に決めるクラスタリング\n",
    "\n",
    "## どのように確率モデルを当てはめるか\n",
    "* 正規分布をはじめとする多くの確率モデルは単峰性の確率分布しか表現できない。\n",
    "* 複数のクラスタがある場合（複数の山がある場合）は、複数の確率モデルの重みつけ線形和で全体の確率分布をモデル化する。\n",
    "* 全体の確率分布は\n",
    "$$ p(\\boldsymbol{x}) = \\sum_{k = 1}^K \\pi_k p_k(\\boldsymbol{x}) $$\n",
    "* このような確率モデルは混合分布モデル\n",
    "* 正規分布がよく使われる。その場合は混合正規分布モデルと呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.4.1. 混合正規分布モデル\n",
    "* k番目のクラスタを表す、d次元正規分布関数を考える。(dはベクトルの次元数)\n",
    "$$ \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) = \\frac{1}{(2\\pi)^{1/d} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp \\left(- \\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}_k) \\right) $$\n",
    "                    \n",
    "* 全体の分布はこれらの線形和。(確率モデルなので面積が1になるように、重みを調整している。$\\pi_k$は混合比\n",
    ")\n",
    "$$ p(\\boldsymbol{x})= \\sum_{k = 1}^K \\pi_k \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}),\\;\\; 0 \\leq \\pi_k \\leq 1, \\;\\; \\sum_{k = 1}^K \\pi_k = 1 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 混合正規分布のパラメータ。\n",
    "$$ \\boldsymbol{\\pi} = (\\pi_1, \\ldots , \\pi_K), \\;\\boldsymbol{\\mu} = (\\boldsymbol{\\mu}_1, \\ldots , \\boldsymbol{\\mu}_K), \\;\\boldsymbol{\\Sigma} = (\\boldsymbol{\\Sigma}_1, \\ldots , \\boldsymbol{\\Sigma}) $$\n",
    "\n",
    "#### ある1つのクラスタにおけるパラメータの個数\n",
    "* $\\pi_k$ : あるクラスタ$k$の確率モデルの重み（値）\n",
    "* $\\mu_k$ : あるクラスタ$k$の各次元の平均（d次元）\n",
    "* $\\Sigma_k$ : あるクラスタ$k$の分散共分散行列(dxdの行列。ただし、対角線で反対側は同じ値。((d+1)xd)/2)\n",
    "\n",
    "### 全体でのパラメータの個数\n",
    "$$K + dK + (d + 1) d K / 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.4.2. 隠れ変数と事後確率\n",
    "* 前述の$K$組みの、混合比$\\pi$、平均ベクトル$\\mu$、共分散行列$\\Sigma$を推定する。\n",
    "* ある1つのデータxがどのクラスタに属するかの表現（OneHotベクトル）。隠れたクラスタを指定しているので隠れ変数と呼ぶ。\n",
    "$$\\boldsymbol{z} = (z_1, \\ldots , z_K)^T, \\;\\; \\sum_{k = 1}^K z_k = 1, \\;\\; \\boldsymbol{z} = (0, \\ldots , 0, 1, 0, \\ldots , 0)^T $$\n",
    "                    \n",
    "* 実際に観測される変数xと隠れ変数zの同時分布は（ある1つのクラスタkのデータxが生じる確率は）\n",
    "$$ p(\\boldsymbol{x}, \\boldsymbol{z}) = p(\\boldsymbol{z}) p(\\boldsymbol{x} | \\boldsymbol{z})$$\n",
    "\n",
    "* なお、あるクラスタ$k$である確率は混合比と一緒なので下記。($x$は関係ない)\n",
    "$$p(z_k = 1) = \\pi_k$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* よって隠れ変数の分布は下記。（$z_k$が該当するクラスタ以外の場合は$0$になる）\n",
    "$$ p(\\boldsymbol{z}) = \\prod_{k = 1}^K \\pi_k^{z_k} $$\n",
    "\n",
    "* また、$ p(\\boldsymbol{x} | z_k = 1) = \\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)$ と表せる。(あるクラスタ$k$の分布は前述の正規分布関数で表せる。)\n",
    "* そのため、観測データの隠れ変数による条件付き分布は、\n",
    "                    \n",
    "$$p(\\boldsymbol{x} | \\boldsymbol{z})\n",
    "\t\t\t\t\t= \\prod_{k = 1}^K \\left(\n",
    "\t\t\t\t\t\\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "\t\t\t\t\t\\right)^{z_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* ここまで、$z$はあるクラスタに絞って計算していた。しかし、$p(x)$を求めるには全てのクラスタの場合について計算する必要がある。そのため、同時分布$p(x,z)$を全ての$z$について加えて、\n",
    "$$ p(\\boldsymbol{x}) = \\sum_{k = 1}^K p(\\boldsymbol{z}) p(\\boldsymbol{x}|\\boldsymbol{z}) = \\sum_{k = 1}^K \\prod_{k = 1}^K \\pi_k^{z_k} \\prod_{k = 1}^K ( \\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}_k, \\Sigma_k))^{z_k} = \\sum_{k = 1}^K \\pi_k \\mathcal{N}(\\boldsymbol{x}|\\boldsymbol{\\mu}_k , \\boldsymbol{\\Sigma}_k) $$\n",
    "* （$x$がどのような値をとるかだけを見ており、クラスタは意識していない。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 隠れ変数の事後確率は下記。（あるデータ$x$が、あるクラスタ$k$に属する確率)\n",
    "$$ \\begin{align} p(z_k = 1 | \\boldsymbol{x}) & = \\frac{p(z_k = 1) p(\\boldsymbol{x} | z_k = 1)}{p(\\boldsymbol{x})} = \\frac{p(z_k = 1) p(\\boldsymbol{x} | z_k = 1)}{\\sum_{j = 1}^K p(z_j = 1) p(\\boldsymbol{x} | z_j = 1)} \\\\ & = \\frac{\\pi_k \\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{ \\sum_{j = 1}^K \\pi_j \\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.4.3. 完全データの対数尤度\n",
    "* ここまではある1つの$x$について考えていた。\n",
    "* 実際には観測データは$N$個ある。そのため隠れ変数もN組ある。\n",
    "* 観測データ(dxNの行列)\n",
    "$$\\boldsymbol{X} = (\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_N), \\;\\; \\boldsymbol{x}_i = (x_{i1}, \\ldots, x_{id})^T$$\n",
    "* 隠れ変数(kxNの行列)\n",
    "$$\\boldsymbol{Z} = (\\boldsymbol{z}_1, \\ldots, \\boldsymbol{z}_N), \\;\\; \\boldsymbol{z}_i = (z_{i1}, \\ldots, z_{iK})^T$$\n",
    "* 観測データと隠れ変数を合わせた集合(完全データ)\n",
    "$$ \\boldsymbol{Y} = (\\boldsymbol{x}_1, \\ldots , \\boldsymbol{x}_N, \\boldsymbol{z}_1, \\ldots , \\boldsymbol{z}_N) = (\\boldsymbol{X}, \\boldsymbol{Z}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 混合正規分布モデルのパラメータは、完全データの尤度を最大にするパラメータとなるようにする。(観測データが最も起こりやすいようなパラメータにする。)\n",
    "* 尤度は下記の式。下記の式が最大になるようなパラメータを求めたい。\n",
    "$$ \\begin{align} p(\\boldsymbol{Y}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) & = p(\\boldsymbol{Z}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) p(\\boldsymbol{X}|\\boldsymbol{Z}, \\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\\\\n",
    "&= \\prod_{i = 1}^N \\prod_{k = 1}^K ( \\pi_k \\mathcal{N}(\\boldsymbol{x}_i|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) )^{z_{ik}} \\end{align} $$\n",
    "* 式（10.15),(10,16)を参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 最尤推定値を求めるために対数尤度関数にする。（掛け算のままでは計算が大変なので、対数変換）\n",
    "$$ \\begin{align*} \\tilde{\\mathcal{L}} & = \\ln p(\\boldsymbol{Y}|\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sum_{i = 1}^N \\sum_{k = 1}^K z_{ik} \\ln \\pi_k + ^sum_{i = 1}^N \\sum_{k = 1}^K z_{ik} \\ln \\mathcal{N}(\\boldsymbol{x}_i|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\\\ & = \\sum_{i = 1}^N \\sum_{k = 1}^K z_{ik} \\ln \\pi_k \\\\ & \\;\\;\\; + \\sum_{i = 1}^N \\sum_{k = 1}^K z_{ik} ( -\\frac{d}{2} \\ln (2\\pi) + \\frac{1}{2} \\ln |\\boldsymbol{\\Sigma}_k|^{-1} - \\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k) ) \\end{align*}$$\n",
    "* ただし隠れ変数があるため、最尤推定値を直接求めることはできない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.4.4. Q関数\n",
    "* 完全データの対数尤度の、隠れ変数に関する期待値をとると、\n",
    "$$ \\begin{align*} \\mathcal{L} & = E_z\\{ \\tilde{\\mathcal{L}} \\} =  \\sum_{i = 1}^N \\sum_{k = 1}^K E_{z_{ik}} \\{ z_{ik} \\} \\ln \\pi_k \\\\ & \\;\\;\\; + \\sum_{i = 1}^N \\sum_{k = 1}^K E_{z_{ik}} \\left( - \\frac{d}{2} \\ln (2\\pi) + \\frac{1}{2} \\ln |\\boldsymbol{\\Sigma_k}|^{-1} - \\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1} (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k) \\right) \\end{align*} $$\n",
    "* 隠れ変数に関する期待値は、式(10.18)より、隠れ変数の事後確率と一致する。\n",
    "$$ E_{z_{ik}} \\{ z_{ik} \\} = \\sum_{z_{ik} = \\{0, 1\\}} z_{ik} p(z_{ik}|\\boldsymbol{x}_i, \\pi_k, \\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k) = 1 \\times p(z_{ik} = 1 | \\boldsymbol{x}_i) $$\n",
    "* そこで、式(10.21)の隠れ変数の期待値を事後確率で置き換える。これをQ関数と呼ぶ。これを最大にしたい。\n",
    "$$ \\begin{align*} Q & =  \\sum_{i = 1}^N \\sum_{k = 1}^K p(z_{ik} = 1 | \\boldsymbol{x}_i) \\ln \\pi_k \\\\ & \\;\\;\\; + \\sum_{i = 1}^N \\sum_{k = 1}^K p(z_{ik} = 1 | \\boldsymbol{x}_i) \\left(- \\frac{d}{2} \\ln (2\\pi) + \\frac{1}{2} \\ln |\\boldsymbol{\\Sigma_k}|^{-1} - \\frac{1}{2}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k) \\right) \\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.4.5. EMアルゴリズム\n",
    "* 隠れ変数がある場合に、確率モデルのパラメータの最尤推定値を求める手法\n",
    "\n",
    "### 2つのステップ\n",
    "* Expectatonステップ\n",
    "  * 現在のパラメータの値を用いて、隠れ変数の事後確率の推定を行う。\n",
    "\n",
    "* Maximizationステップ\n",
    "  * Eステップで求めた隠れ変数の事後確率を用いて、Q関数に代入し、Q関数を最大にするパラメータ（微分がゼロになるパラメータ)を求める。\n",
    "  \n",
    "* この2つのステップを、完全データの対数尤度に変化がなくなるまで繰り返すことで、パラメータの最尤推定値を得る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## アルゴリズム\n",
    "1. 3つのパラメータ$\\pi_k, \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k$を初期化\n",
    "2. Eステップ：現在のパラメータを用いて、隠れ変数の事後確率の推定を行う。\n",
    "$$ \\gamma (z_{ik}) =  \\frac{\\pi_k \\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j = 1}^K \\pi_j \\mathcal{N}(\\boldsymbol{x} | \\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)}$$\n",
    "3. Mステップ：3つのパラメータを更新する。Q関数に$\\gamma (z_ik)$ を代入し、Q関数が最大となるようなパラメータに変更する。\n",
    "  * パラメータの計算のために、$k$番目のクラスタに属するデータ数の推定値を求める。（ある点がクラスタ$k$に属するデータの確率を足し合わせる）\n",
    "$$ N_k = \\sum_{i = 1}^N \\gamma (z_{ik})$$ \n",
    "  * $\\mu_k$ : ある点$x_i$が$k$クラスタになる確率をかける。それを全ての$x$について行い足し合わせ、$k$クラスタのデータ数の推定値で割ると平均になる。\n",
    "$$ \\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{i = 1}^N \\gamma (z_{ik})　\\boldsymbol{x}_i $$\n",
    "  * $\\boldsymbol{\\Sigma}_k$ : \n",
    "$$ \\boldsymbol{\\Sigma}_k = \\frac{\\sum_{i = 1}^N \\gamma (z_{ik}) (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k) (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^T}{N_k} $$\n",
    "  * $\\pi_k$ : \n",
    "  $$\\pi_k = \\frac{N_k}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.4.6. Mステップの導出\n",
    "* MステップはQ関数を各パラメータで微分して0とおいて導出することができる。\n",
    "* 以下略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 10.4.7.EMアルゴリズムの性質\n",
    "* 混合正規分布のEMアルゴリズムについて一般的に解釈する。\n",
    "* $ \\boldsymbol{ \\theta } $の時、観測データ $ \\boldsymbol{ X } $の尤度は、隠れ変数$ \\boldsymbol{ Z } $ を含めて表現すれば$$ p( \\boldsymbol{ X } | \\boldsymbol{ \\theta } )= \\frac{ p( \\boldsymbol{ X }, \\boldsymbol{ Z } | \\boldsymbol{ \\theta } ) }{p( \\boldsymbol{ Z } | \\boldsymbol{ X }, \\boldsymbol{ \\theta } ) } $$\n",
    "なぜなら\n",
    "$$ p( \\boldsymbol{ Z } | \\boldsymbol{ X }, \\boldsymbol{ \\theta } )  p( \\boldsymbol{ X } | \\boldsymbol{ \\theta } ) = p( \\boldsymbol{ X }, \\boldsymbol{ Z } | \\boldsymbol{ \\theta } ) $$\n",
    "* ここで、隠れ変数$ \\boldsymbol{ Z } $に関する任意の分布$q(Z)$を考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 観測データの対数尤度は、\n",
    "\n",
    "$$ \\begin{align*} \\ln{ p( \\boldsymbol{ X } | \\boldsymbol{ \\theta } ) } & = \\sum_{ \\boldsymbol{ z } } q( \\boldsymbol{ Z } ) \\ln{ p( \\boldsymbol{ X } | \\boldsymbol{ \\theta } ) } = \\sum_{ \\boldsymbol{ z } } \\ln{ \\frac{ p( \\boldsymbol{ X }, \\boldsymbol{ Z } | \\boldsymbol{ \\theta } ) }{ p( \\boldsymbol{ Z } | \\boldsymbol{ X }, \\boldsymbol{ \\theta } ) } } \\\\ & = \\sum_{ \\boldsymbol{ z } } q( \\boldsymbol{ Z } ) \\ln{ \\left( \\frac{ p( \\boldsymbol{ X }, \\boldsymbol{ Z } | \\boldsymbol{ \\theta } ) }{ q ( \\boldsymbol{ Z } ) } \\frac{ q ( \\boldsymbol{ Z } ) }{ p( \\boldsymbol{ Z } | \\boldsymbol{ X }, \\boldsymbol{ \\theta } ) }\n",
    "\\right) } \\\\ & = \\sum_{ \\boldsymbol{ z } } \\ln{ \\frac{ p( \\boldsymbol{ X }, \\boldsymbol{ Z } | \\boldsymbol{ \\theta } ) }{ q ( \\boldsymbol{ Z } ) } } + \\sum_{ \\boldsymbol{ z } } q( \\boldsymbol{ Z }) \\ln{ \\frac{ q ( \\boldsymbol{ Z } ) }{ p( \\boldsymbol{ Z } | \\boldsymbol{ X }, \\boldsymbol{ \\theta } ) } }\\end{align*} $$\n",
    "\n",
    "* 第1項、第2項をそれぞれ置き換えて、\n",
    "$$ \\ln{ p( \\boldsymbol{ X } | \\boldsymbol{ \\theta } ) } = \\mathcal{ L } (q, \\boldsymbol{ \\theta }) + \\mathtt{ KL }(q || p) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* 第2項の$ \\mathtt{ KL }(q || p) $ はカルバック-ライブラー情報量と呼ばれる。\n",
    "  * $ q( \\boldsymbol{ Z } ) $ と事後分布$ p( \\boldsymbol{ Z } | \\boldsymbol{ X }, \\boldsymbol{ \\theta } ) $ の2つの確率分布間の距離を与える量。常に正の値。\n",
    "* $ \\mathcal{ L } ( q, \\boldsymbol{ \\theta } ) $ は、観測データの対数尤度 $ \\ln p( \\boldsymbol{ X } | \\boldsymbol{ \\theta } ) $の下限を与えていることになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* EMアルゴリズムは $ \\ln p( \\boldsymbol{ X } | \\boldsymbol{ \\theta } ) $ を逐次最大化する。\n",
    "* Eステップ\n",
    "  * $t$回目で求めた $ \\boldsymbol{ \\theta }^{(t)} $ を固定して、$ \\mathcal{ L } ( q, \\boldsymbol{ \\theta } ) $ を $q$に関して最大化する。\n",
    "* Mステップ\n",
    "  * 最大化された $q^{(t)}$を用いて、$ \\mathcal{ L } ( q, \\boldsymbol{ \\theta } ) $ を $ \\boldsymbol{ \\theta } $ に関して最大化することで、 $ \\mathtt{ KL }(q || p) $ を小さくし、隠れ変数の分布を真の分布に近づける。\n",
    "* 更新により、Q関数の値は常に増加するか収束する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 参考資料\n",
    "* [はじめてのパターン認識](https://amzn.to/2Gjf0CU)\n",
    "* [2GMon/haji_pata はじめてのパターン認識 勉強メモ](https://github.com/2GMon/haji_pata)\n",
    "* [EMアルゴリズム徹底解説](https://qiita.com/kenmatsu4/items/59ea3e5dfa3d4c161efb)\n",
    "* [Jupyter Notebookでプレゼンをするとっても便利な方法](https://qiita.com/cvusk/items/d425751ba663dc8c6517)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
